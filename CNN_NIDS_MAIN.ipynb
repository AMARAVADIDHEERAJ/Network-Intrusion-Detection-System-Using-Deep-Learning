{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ba7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e898a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv(r\"C:\\Users\\laksh\\VITC_Projects\\2. NIDS_Prediction\\Total_CSVs\\NIDS_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32697be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ip_1</th>\n",
       "      <th>ip_144</th>\n",
       "      <th>ip_204</th>\n",
       "      <th>ip_205</th>\n",
       "      <th>ip_207</th>\n",
       "      <th>ip_8</th>\n",
       "      <th>not_freq</th>\n",
       "      <th>ip_4</th>\n",
       "      <th>ip_11</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>r_time_var</th>\n",
       "      <th>r_time_std</th>\n",
       "      <th>r_time_mean</th>\n",
       "      <th>r_time_med</th>\n",
       "      <th>r_time_mode</th>\n",
       "      <th>r_s_med_time</th>\n",
       "      <th>r_s_mode_time</th>\n",
       "      <th>r_c_time</th>\n",
       "      <th>Benign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.135513</td>\n",
       "      <td>-0.178935</td>\n",
       "      <td>-0.242217</td>\n",
       "      <td>-0.237193</td>\n",
       "      <td>-0.218589</td>\n",
       "      <td>-0.159629</td>\n",
       "      <td>-0.807492</td>\n",
       "      <td>0.773925</td>\n",
       "      <td>-0.388178</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.882221</td>\n",
       "      <td>-0.178930</td>\n",
       "      <td>-0.240318</td>\n",
       "      <td>-0.239660</td>\n",
       "      <td>-0.221809</td>\n",
       "      <td>-0.159613</td>\n",
       "      <td>-0.153837</td>\n",
       "      <td>0.146645</td>\n",
       "      <td>-0.157056</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200209</td>\n",
       "      <td>-0.156441</td>\n",
       "      <td>0.115037</td>\n",
       "      <td>-0.217159</td>\n",
       "      <td>-0.221747</td>\n",
       "      <td>-0.159634</td>\n",
       "      <td>0.447061</td>\n",
       "      <td>-0.399077</td>\n",
       "      <td>2.175369</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788347</td>\n",
       "      <td>-0.178938</td>\n",
       "      <td>-0.243053</td>\n",
       "      <td>-0.241114</td>\n",
       "      <td>-0.225992</td>\n",
       "      <td>-0.159633</td>\n",
       "      <td>1.398031</td>\n",
       "      <td>0.092140</td>\n",
       "      <td>-0.112256</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678286</td>\n",
       "      <td>-0.178946</td>\n",
       "      <td>-0.250232</td>\n",
       "      <td>5.245053</td>\n",
       "      <td>4.540288</td>\n",
       "      <td>6.300846</td>\n",
       "      <td>-5.075614</td>\n",
       "      <td>-7.198931</td>\n",
       "      <td>-0.632209</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ip_1  ip_144  ip_204  ip_205  ip_207  ip_8  not_freq  ip_4  \\\n",
       "0           0     0       0       0       0       0     0         1     0   \n",
       "1           1     0       0       0       0       0     0         1     0   \n",
       "2           2     0       0       0       0       0     0         1     0   \n",
       "3           3     0       0       0       0       0     0         1     0   \n",
       "4           4     0       0       0       0       0     0         1     0   \n",
       "\n",
       "   ip_11  ...         0  r_time_var  r_time_std  r_time_mean  r_time_med  \\\n",
       "0      0  ... -1.135513   -0.178935   -0.242217    -0.237193   -0.218589   \n",
       "1      0  ... -0.882221   -0.178930   -0.240318    -0.239660   -0.221809   \n",
       "2      0  ...  0.200209   -0.156441    0.115037    -0.217159   -0.221747   \n",
       "3      0  ... -0.788347   -0.178938   -0.243053    -0.241114   -0.225992   \n",
       "4      0  ... -0.678286   -0.178946   -0.250232     5.245053    4.540288   \n",
       "\n",
       "   r_time_mode  r_s_med_time  r_s_mode_time  r_c_time  Benign  \n",
       "0    -0.159629     -0.807492       0.773925 -0.388178  Benign  \n",
       "1    -0.159613     -0.153837       0.146645 -0.157056  Benign  \n",
       "2    -0.159634      0.447061      -0.399077  2.175369  Benign  \n",
       "3    -0.159633      1.398031       0.092140 -0.112256  Benign  \n",
       "4     6.300846     -5.075614      -7.198931 -0.632209  Benign  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6dbb1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\laksh\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\laksh\\anaconda3\\lib\\site-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\laksh\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\laksh\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\laksh\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.20.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\laksh\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\laksh\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ccf4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef64630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db.iloc[:,1:46]\n",
    "db = db.iloc[:,0:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cdba4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip_1</th>\n",
       "      <th>ip_144</th>\n",
       "      <th>ip_204</th>\n",
       "      <th>ip_205</th>\n",
       "      <th>ip_207</th>\n",
       "      <th>ip_8</th>\n",
       "      <th>not_freq</th>\n",
       "      <th>ip_4</th>\n",
       "      <th>ip_11</th>\n",
       "      <th>dest_ip_130_1_8</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>r_time_var</th>\n",
       "      <th>r_time_std</th>\n",
       "      <th>r_time_mean</th>\n",
       "      <th>r_time_med</th>\n",
       "      <th>r_time_mode</th>\n",
       "      <th>r_s_med_time</th>\n",
       "      <th>r_s_mode_time</th>\n",
       "      <th>r_c_time</th>\n",
       "      <th>Benign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.135513</td>\n",
       "      <td>-0.178935</td>\n",
       "      <td>-0.242217</td>\n",
       "      <td>-0.237193</td>\n",
       "      <td>-0.218589</td>\n",
       "      <td>-0.159629</td>\n",
       "      <td>-0.807492</td>\n",
       "      <td>0.773925</td>\n",
       "      <td>-0.388178</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.882221</td>\n",
       "      <td>-0.178930</td>\n",
       "      <td>-0.240318</td>\n",
       "      <td>-0.239660</td>\n",
       "      <td>-0.221809</td>\n",
       "      <td>-0.159613</td>\n",
       "      <td>-0.153837</td>\n",
       "      <td>0.146645</td>\n",
       "      <td>-0.157056</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200209</td>\n",
       "      <td>-0.156441</td>\n",
       "      <td>0.115037</td>\n",
       "      <td>-0.217159</td>\n",
       "      <td>-0.221747</td>\n",
       "      <td>-0.159634</td>\n",
       "      <td>0.447061</td>\n",
       "      <td>-0.399077</td>\n",
       "      <td>2.175369</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788347</td>\n",
       "      <td>-0.178938</td>\n",
       "      <td>-0.243053</td>\n",
       "      <td>-0.241114</td>\n",
       "      <td>-0.225992</td>\n",
       "      <td>-0.159633</td>\n",
       "      <td>1.398031</td>\n",
       "      <td>0.092140</td>\n",
       "      <td>-0.112256</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678286</td>\n",
       "      <td>-0.178946</td>\n",
       "      <td>-0.250232</td>\n",
       "      <td>5.245053</td>\n",
       "      <td>4.540288</td>\n",
       "      <td>6.300846</td>\n",
       "      <td>-5.075614</td>\n",
       "      <td>-7.198931</td>\n",
       "      <td>-0.632209</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269638</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748533</td>\n",
       "      <td>-0.178941</td>\n",
       "      <td>-0.244679</td>\n",
       "      <td>-0.239679</td>\n",
       "      <td>-0.221606</td>\n",
       "      <td>-0.159583</td>\n",
       "      <td>-0.624305</td>\n",
       "      <td>0.650562</td>\n",
       "      <td>-0.364881</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269639</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.119040</td>\n",
       "      <td>-0.178929</td>\n",
       "      <td>-0.240170</td>\n",
       "      <td>-0.238542</td>\n",
       "      <td>-0.218699</td>\n",
       "      <td>-0.159629</td>\n",
       "      <td>-1.036147</td>\n",
       "      <td>0.308279</td>\n",
       "      <td>-0.249549</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269640</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.888898</td>\n",
       "      <td>-0.178946</td>\n",
       "      <td>-0.250202</td>\n",
       "      <td>-0.243936</td>\n",
       "      <td>-0.226444</td>\n",
       "      <td>-0.159625</td>\n",
       "      <td>0.234809</td>\n",
       "      <td>0.157984</td>\n",
       "      <td>-0.356937</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269641</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900145</td>\n",
       "      <td>-0.178939</td>\n",
       "      <td>-0.243902</td>\n",
       "      <td>-0.240051</td>\n",
       "      <td>-0.221605</td>\n",
       "      <td>-0.159626</td>\n",
       "      <td>-0.732100</td>\n",
       "      <td>0.427483</td>\n",
       "      <td>-0.298511</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269642</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.051665</td>\n",
       "      <td>-0.178929</td>\n",
       "      <td>-0.240134</td>\n",
       "      <td>-0.238340</td>\n",
       "      <td>-0.219059</td>\n",
       "      <td>-0.159630</td>\n",
       "      <td>-0.810333</td>\n",
       "      <td>0.335843</td>\n",
       "      <td>-0.261945</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269643 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ip_1  ip_144  ip_204  ip_205  ip_207  ip_8  not_freq  ip_4  ip_11  \\\n",
       "0          0       0       0       0       0     0         1     0      0   \n",
       "1          0       0       0       0       0     0         1     0      0   \n",
       "2          0       0       0       0       0     0         1     0      0   \n",
       "3          0       0       0       0       0     0         1     0      0   \n",
       "4          0       0       0       0       0     0         1     0      0   \n",
       "...      ...     ...     ...     ...     ...   ...       ...   ...    ...   \n",
       "269638     1       0       0       0       0     0         0     0      0   \n",
       "269639     1       0       0       0       0     0         0     0      0   \n",
       "269640     1       0       0       0       0     0         0     0      0   \n",
       "269641     1       0       0       0       0     0         0     0      0   \n",
       "269642     1       0       0       0       0     0         0     0      0   \n",
       "\n",
       "        dest_ip_130_1_8  ...         0  r_time_var  r_time_std  r_time_mean  \\\n",
       "0                     0  ... -1.135513   -0.178935   -0.242217    -0.237193   \n",
       "1                     0  ... -0.882221   -0.178930   -0.240318    -0.239660   \n",
       "2                     0  ...  0.200209   -0.156441    0.115037    -0.217159   \n",
       "3                     0  ... -0.788347   -0.178938   -0.243053    -0.241114   \n",
       "4                     0  ... -0.678286   -0.178946   -0.250232     5.245053   \n",
       "...                 ...  ...       ...         ...         ...          ...   \n",
       "269638                0  ...  0.748533   -0.178941   -0.244679    -0.239679   \n",
       "269639                0  ... -1.119040   -0.178929   -0.240170    -0.238542   \n",
       "269640                0  ...  1.888898   -0.178946   -0.250202    -0.243936   \n",
       "269641                0  ...  0.900145   -0.178939   -0.243902    -0.240051   \n",
       "269642                0  ... -1.051665   -0.178929   -0.240134    -0.238340   \n",
       "\n",
       "        r_time_med  r_time_mode  r_s_med_time  r_s_mode_time  r_c_time  \\\n",
       "0        -0.218589    -0.159629     -0.807492       0.773925 -0.388178   \n",
       "1        -0.221809    -0.159613     -0.153837       0.146645 -0.157056   \n",
       "2        -0.221747    -0.159634      0.447061      -0.399077  2.175369   \n",
       "3        -0.225992    -0.159633      1.398031       0.092140 -0.112256   \n",
       "4         4.540288     6.300846     -5.075614      -7.198931 -0.632209   \n",
       "...            ...          ...           ...            ...       ...   \n",
       "269638   -0.221606    -0.159583     -0.624305       0.650562 -0.364881   \n",
       "269639   -0.218699    -0.159629     -1.036147       0.308279 -0.249549   \n",
       "269640   -0.226444    -0.159625      0.234809       0.157984 -0.356937   \n",
       "269641   -0.221605    -0.159626     -0.732100       0.427483 -0.298511   \n",
       "269642   -0.219059    -0.159630     -0.810333       0.335843 -0.261945   \n",
       "\n",
       "           Benign  \n",
       "0          Benign  \n",
       "1          Benign  \n",
       "2          Benign  \n",
       "3          Benign  \n",
       "4          Benign  \n",
       "...           ...  \n",
       "269638  Malicious  \n",
       "269639  Malicious  \n",
       "269640  Malicious  \n",
       "269641  Malicious  \n",
       "269642  Malicious  \n",
       "\n",
       "[269643 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bda51ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip_1</th>\n",
       "      <th>ip_144</th>\n",
       "      <th>ip_204</th>\n",
       "      <th>ip_205</th>\n",
       "      <th>ip_207</th>\n",
       "      <th>ip_8</th>\n",
       "      <th>not_freq</th>\n",
       "      <th>ip_4</th>\n",
       "      <th>ip_11</th>\n",
       "      <th>dest_ip_130_1_8</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>r_time_var</th>\n",
       "      <th>r_time_std</th>\n",
       "      <th>r_time_mean</th>\n",
       "      <th>r_time_med</th>\n",
       "      <th>r_time_mode</th>\n",
       "      <th>r_s_med_time</th>\n",
       "      <th>r_s_mode_time</th>\n",
       "      <th>r_c_time</th>\n",
       "      <th>Benign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.135513</td>\n",
       "      <td>-0.178935</td>\n",
       "      <td>-0.242217</td>\n",
       "      <td>-0.237193</td>\n",
       "      <td>-0.218589</td>\n",
       "      <td>-0.159629</td>\n",
       "      <td>-0.807492</td>\n",
       "      <td>0.773925</td>\n",
       "      <td>-0.388178</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.882221</td>\n",
       "      <td>-0.178930</td>\n",
       "      <td>-0.240318</td>\n",
       "      <td>-0.239660</td>\n",
       "      <td>-0.221809</td>\n",
       "      <td>-0.159613</td>\n",
       "      <td>-0.153837</td>\n",
       "      <td>0.146645</td>\n",
       "      <td>-0.157056</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200209</td>\n",
       "      <td>-0.156441</td>\n",
       "      <td>0.115037</td>\n",
       "      <td>-0.217159</td>\n",
       "      <td>-0.221747</td>\n",
       "      <td>-0.159634</td>\n",
       "      <td>0.447061</td>\n",
       "      <td>-0.399077</td>\n",
       "      <td>2.175369</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788347</td>\n",
       "      <td>-0.178938</td>\n",
       "      <td>-0.243053</td>\n",
       "      <td>-0.241114</td>\n",
       "      <td>-0.225992</td>\n",
       "      <td>-0.159633</td>\n",
       "      <td>1.398031</td>\n",
       "      <td>0.092140</td>\n",
       "      <td>-0.112256</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678286</td>\n",
       "      <td>-0.178946</td>\n",
       "      <td>-0.250232</td>\n",
       "      <td>5.245053</td>\n",
       "      <td>4.540288</td>\n",
       "      <td>6.300846</td>\n",
       "      <td>-5.075614</td>\n",
       "      <td>-7.198931</td>\n",
       "      <td>-0.632209</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269638</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748533</td>\n",
       "      <td>-0.178941</td>\n",
       "      <td>-0.244679</td>\n",
       "      <td>-0.239679</td>\n",
       "      <td>-0.221606</td>\n",
       "      <td>-0.159583</td>\n",
       "      <td>-0.624305</td>\n",
       "      <td>0.650562</td>\n",
       "      <td>-0.364881</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269639</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.119040</td>\n",
       "      <td>-0.178929</td>\n",
       "      <td>-0.240170</td>\n",
       "      <td>-0.238542</td>\n",
       "      <td>-0.218699</td>\n",
       "      <td>-0.159629</td>\n",
       "      <td>-1.036147</td>\n",
       "      <td>0.308279</td>\n",
       "      <td>-0.249549</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269640</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.888898</td>\n",
       "      <td>-0.178946</td>\n",
       "      <td>-0.250202</td>\n",
       "      <td>-0.243936</td>\n",
       "      <td>-0.226444</td>\n",
       "      <td>-0.159625</td>\n",
       "      <td>0.234809</td>\n",
       "      <td>0.157984</td>\n",
       "      <td>-0.356937</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269641</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900145</td>\n",
       "      <td>-0.178939</td>\n",
       "      <td>-0.243902</td>\n",
       "      <td>-0.240051</td>\n",
       "      <td>-0.221605</td>\n",
       "      <td>-0.159626</td>\n",
       "      <td>-0.732100</td>\n",
       "      <td>0.427483</td>\n",
       "      <td>-0.298511</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269642</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.051665</td>\n",
       "      <td>-0.178929</td>\n",
       "      <td>-0.240134</td>\n",
       "      <td>-0.238340</td>\n",
       "      <td>-0.219059</td>\n",
       "      <td>-0.159630</td>\n",
       "      <td>-0.810333</td>\n",
       "      <td>0.335843</td>\n",
       "      <td>-0.261945</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269643 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ip_1  ip_144  ip_204  ip_205  ip_207  ip_8  not_freq  ip_4  ip_11  \\\n",
       "0          0       0       0       0       0     0         1     0      0   \n",
       "1          0       0       0       0       0     0         1     0      0   \n",
       "2          0       0       0       0       0     0         1     0      0   \n",
       "3          0       0       0       0       0     0         1     0      0   \n",
       "4          0       0       0       0       0     0         1     0      0   \n",
       "...      ...     ...     ...     ...     ...   ...       ...   ...    ...   \n",
       "269638     1       0       0       0       0     0         0     0      0   \n",
       "269639     1       0       0       0       0     0         0     0      0   \n",
       "269640     1       0       0       0       0     0         0     0      0   \n",
       "269641     1       0       0       0       0     0         0     0      0   \n",
       "269642     1       0       0       0       0     0         0     0      0   \n",
       "\n",
       "        dest_ip_130_1_8  ...         0  r_time_var  r_time_std  r_time_mean  \\\n",
       "0                     0  ... -1.135513   -0.178935   -0.242217    -0.237193   \n",
       "1                     0  ... -0.882221   -0.178930   -0.240318    -0.239660   \n",
       "2                     0  ...  0.200209   -0.156441    0.115037    -0.217159   \n",
       "3                     0  ... -0.788347   -0.178938   -0.243053    -0.241114   \n",
       "4                     0  ... -0.678286   -0.178946   -0.250232     5.245053   \n",
       "...                 ...  ...       ...         ...         ...          ...   \n",
       "269638                0  ...  0.748533   -0.178941   -0.244679    -0.239679   \n",
       "269639                0  ... -1.119040   -0.178929   -0.240170    -0.238542   \n",
       "269640                0  ...  1.888898   -0.178946   -0.250202    -0.243936   \n",
       "269641                0  ...  0.900145   -0.178939   -0.243902    -0.240051   \n",
       "269642                0  ... -1.051665   -0.178929   -0.240134    -0.238340   \n",
       "\n",
       "        r_time_med  r_time_mode  r_s_med_time  r_s_mode_time  r_c_time  \\\n",
       "0        -0.218589    -0.159629     -0.807492       0.773925 -0.388178   \n",
       "1        -0.221809    -0.159613     -0.153837       0.146645 -0.157056   \n",
       "2        -0.221747    -0.159634      0.447061      -0.399077  2.175369   \n",
       "3        -0.225992    -0.159633      1.398031       0.092140 -0.112256   \n",
       "4         4.540288     6.300846     -5.075614      -7.198931 -0.632209   \n",
       "...            ...          ...           ...            ...       ...   \n",
       "269638   -0.221606    -0.159583     -0.624305       0.650562 -0.364881   \n",
       "269639   -0.218699    -0.159629     -1.036147       0.308279 -0.249549   \n",
       "269640   -0.226444    -0.159625      0.234809       0.157984 -0.356937   \n",
       "269641   -0.221605    -0.159626     -0.732100       0.427483 -0.298511   \n",
       "269642   -0.219059    -0.159630     -0.810333       0.335843 -0.261945   \n",
       "\n",
       "           Benign  \n",
       "0          Benign  \n",
       "1          Benign  \n",
       "2          Benign  \n",
       "3          Benign  \n",
       "4          Benign  \n",
       "...           ...  \n",
       "269638  Malicious  \n",
       "269639  Malicious  \n",
       "269640  Malicious  \n",
       "269641  Malicious  \n",
       "269642  Malicious  \n",
       "\n",
       "[269643 rows x 45 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = db.iloc[:,0:45]\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09ad49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip_1</th>\n",
       "      <th>ip_144</th>\n",
       "      <th>ip_204</th>\n",
       "      <th>ip_205</th>\n",
       "      <th>ip_207</th>\n",
       "      <th>ip_8</th>\n",
       "      <th>not_freq</th>\n",
       "      <th>ip_4</th>\n",
       "      <th>ip_11</th>\n",
       "      <th>dest_ip_130_1_8</th>\n",
       "      <th>...</th>\n",
       "      <th>s_mode_time</th>\n",
       "      <th>0</th>\n",
       "      <th>r_time_var</th>\n",
       "      <th>r_time_std</th>\n",
       "      <th>r_time_mean</th>\n",
       "      <th>r_time_med</th>\n",
       "      <th>r_time_mode</th>\n",
       "      <th>r_s_med_time</th>\n",
       "      <th>r_s_mode_time</th>\n",
       "      <th>r_c_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.562871</td>\n",
       "      <td>-1.135513</td>\n",
       "      <td>-0.178935</td>\n",
       "      <td>-0.242217</td>\n",
       "      <td>-0.237193</td>\n",
       "      <td>-0.218589</td>\n",
       "      <td>-0.159629</td>\n",
       "      <td>-0.807492</td>\n",
       "      <td>0.773925</td>\n",
       "      <td>-0.388178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.909475</td>\n",
       "      <td>-0.882221</td>\n",
       "      <td>-0.178930</td>\n",
       "      <td>-0.240318</td>\n",
       "      <td>-0.239660</td>\n",
       "      <td>-0.221809</td>\n",
       "      <td>-0.159613</td>\n",
       "      <td>-0.153837</td>\n",
       "      <td>0.146645</td>\n",
       "      <td>-0.157056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.381118</td>\n",
       "      <td>0.200209</td>\n",
       "      <td>-0.156441</td>\n",
       "      <td>0.115037</td>\n",
       "      <td>-0.217159</td>\n",
       "      <td>-0.221747</td>\n",
       "      <td>-0.159634</td>\n",
       "      <td>0.447061</td>\n",
       "      <td>-0.399077</td>\n",
       "      <td>2.175369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617898</td>\n",
       "      <td>-0.788347</td>\n",
       "      <td>-0.178938</td>\n",
       "      <td>-0.243053</td>\n",
       "      <td>-0.241114</td>\n",
       "      <td>-0.225992</td>\n",
       "      <td>-0.159633</td>\n",
       "      <td>1.398031</td>\n",
       "      <td>0.092140</td>\n",
       "      <td>-0.112256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428250</td>\n",
       "      <td>-0.678286</td>\n",
       "      <td>-0.178946</td>\n",
       "      <td>-0.250232</td>\n",
       "      <td>5.245053</td>\n",
       "      <td>4.540288</td>\n",
       "      <td>6.300846</td>\n",
       "      <td>-5.075614</td>\n",
       "      <td>-7.198931</td>\n",
       "      <td>-0.632209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269638</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.616616</td>\n",
       "      <td>0.748533</td>\n",
       "      <td>-0.178941</td>\n",
       "      <td>-0.244679</td>\n",
       "      <td>-0.239679</td>\n",
       "      <td>-0.221606</td>\n",
       "      <td>-0.159583</td>\n",
       "      <td>-0.624305</td>\n",
       "      <td>0.650562</td>\n",
       "      <td>-0.364881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269639</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.499615</td>\n",
       "      <td>-1.119040</td>\n",
       "      <td>-0.178929</td>\n",
       "      <td>-0.240170</td>\n",
       "      <td>-0.238542</td>\n",
       "      <td>-0.218699</td>\n",
       "      <td>-0.159629</td>\n",
       "      <td>-1.036147</td>\n",
       "      <td>0.308279</td>\n",
       "      <td>-0.249549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269640</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.889490</td>\n",
       "      <td>1.888898</td>\n",
       "      <td>-0.178946</td>\n",
       "      <td>-0.250202</td>\n",
       "      <td>-0.243936</td>\n",
       "      <td>-0.226444</td>\n",
       "      <td>-0.159625</td>\n",
       "      <td>0.234809</td>\n",
       "      <td>0.157984</td>\n",
       "      <td>-0.356937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269641</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.665595</td>\n",
       "      <td>0.900145</td>\n",
       "      <td>-0.178939</td>\n",
       "      <td>-0.243902</td>\n",
       "      <td>-0.240051</td>\n",
       "      <td>-0.221605</td>\n",
       "      <td>-0.159626</td>\n",
       "      <td>-0.732100</td>\n",
       "      <td>0.427483</td>\n",
       "      <td>-0.298511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269642</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.265768</td>\n",
       "      <td>-1.051665</td>\n",
       "      <td>-0.178929</td>\n",
       "      <td>-0.240134</td>\n",
       "      <td>-0.238340</td>\n",
       "      <td>-0.219059</td>\n",
       "      <td>-0.159630</td>\n",
       "      <td>-0.810333</td>\n",
       "      <td>0.335843</td>\n",
       "      <td>-0.261945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269643 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ip_1  ip_144  ip_204  ip_205  ip_207  ip_8  not_freq  ip_4  ip_11  \\\n",
       "0          0       0       0       0       0     0         1     0      0   \n",
       "1          0       0       0       0       0     0         1     0      0   \n",
       "2          0       0       0       0       0     0         1     0      0   \n",
       "3          0       0       0       0       0     0         1     0      0   \n",
       "4          0       0       0       0       0     0         1     0      0   \n",
       "...      ...     ...     ...     ...     ...   ...       ...   ...    ...   \n",
       "269638     1       0       0       0       0     0         0     0      0   \n",
       "269639     1       0       0       0       0     0         0     0      0   \n",
       "269640     1       0       0       0       0     0         0     0      0   \n",
       "269641     1       0       0       0       0     0         0     0      0   \n",
       "269642     1       0       0       0       0     0         0     0      0   \n",
       "\n",
       "        dest_ip_130_1_8  ...  s_mode_time         0  r_time_var  r_time_std  \\\n",
       "0                     0  ...     1.562871 -1.135513   -0.178935   -0.242217   \n",
       "1                     0  ...    -2.909475 -0.882221   -0.178930   -0.240318   \n",
       "2                     0  ...    -0.381118  0.200209   -0.156441    0.115037   \n",
       "3                     0  ...     0.617898 -0.788347   -0.178938   -0.243053   \n",
       "4                     0  ...     0.428250 -0.678286   -0.178946   -0.250232   \n",
       "...                 ...  ...          ...       ...         ...         ...   \n",
       "269638                0  ...    -0.616616  0.748533   -0.178941   -0.244679   \n",
       "269639                0  ...     1.499615 -1.119040   -0.178929   -0.240170   \n",
       "269640                0  ...    -0.889490  1.888898   -0.178946   -0.250202   \n",
       "269641                0  ...    -0.665595  0.900145   -0.178939   -0.243902   \n",
       "269642                0  ...     1.265768 -1.051665   -0.178929   -0.240134   \n",
       "\n",
       "        r_time_mean  r_time_med  r_time_mode  r_s_med_time  r_s_mode_time  \\\n",
       "0         -0.237193   -0.218589    -0.159629     -0.807492       0.773925   \n",
       "1         -0.239660   -0.221809    -0.159613     -0.153837       0.146645   \n",
       "2         -0.217159   -0.221747    -0.159634      0.447061      -0.399077   \n",
       "3         -0.241114   -0.225992    -0.159633      1.398031       0.092140   \n",
       "4          5.245053    4.540288     6.300846     -5.075614      -7.198931   \n",
       "...             ...         ...          ...           ...            ...   \n",
       "269638    -0.239679   -0.221606    -0.159583     -0.624305       0.650562   \n",
       "269639    -0.238542   -0.218699    -0.159629     -1.036147       0.308279   \n",
       "269640    -0.243936   -0.226444    -0.159625      0.234809       0.157984   \n",
       "269641    -0.240051   -0.221605    -0.159626     -0.732100       0.427483   \n",
       "269642    -0.238340   -0.219059    -0.159630     -0.810333       0.335843   \n",
       "\n",
       "        r_c_time  \n",
       "0      -0.388178  \n",
       "1      -0.157056  \n",
       "2       2.175369  \n",
       "3      -0.112256  \n",
       "4      -0.632209  \n",
       "...          ...  \n",
       "269638 -0.364881  \n",
       "269639 -0.249549  \n",
       "269640 -0.356937  \n",
       "269641 -0.298511  \n",
       "269642 -0.261945  \n",
       "\n",
       "[269643 rows x 44 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = db.iloc[:,0:44]\n",
    "y1 = db.iloc[:,44]\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6672005d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ip_1                    False\n",
       "ip_144                  False\n",
       "ip_204                  False\n",
       "ip_205                  False\n",
       "ip_207                  False\n",
       "ip_8                    False\n",
       "not_freq                False\n",
       "ip_4                    False\n",
       "ip_11                   False\n",
       "dest_ip_130_1_8         False\n",
       "not_dest_freq           False\n",
       "Wrong_port              False\n",
       "port_is_443             False\n",
       "Wrong_port.1            False\n",
       "destination_port_443    False\n",
       "Duration                False\n",
       "fs_1807                 False\n",
       "fs_more                 False\n",
       "fsless                  False\n",
       "FlowSentRate            False\n",
       "FlowRecvRate            False\n",
       "Pcket_lenVar            False\n",
       "Length_std              False\n",
       "Length_mean             False\n",
       "L_median                False\n",
       "L_mode                  False\n",
       "s_median                False\n",
       "s_mode                  False\n",
       "PLCV                    False\n",
       "PTV                     False\n",
       "PT_std                  False\n",
       "PT_mean                 False\n",
       "PT_median               False\n",
       "Packt_Time_mode         False\n",
       "s_mode_time             False\n",
       "0                       False\n",
       "r_time_var              False\n",
       "r_time_std              False\n",
       "r_time_mean             False\n",
       "r_time_med              False\n",
       "r_time_mode             False\n",
       "r_s_med_time            False\n",
       "r_s_mode_time           False\n",
       "r_c_time                False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(x1).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb9e8b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Benign\n",
       "1    Benign\n",
       "2    Benign\n",
       "3    Benign\n",
       "4    Benign\n",
       "Name: Benign, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4a0c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTEENN()\n",
    "X_resampled, y_resampled = sm.fit_resample(x1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a9ecbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ip_1                    473122\n",
       "ip_144                  473122\n",
       "ip_204                  473122\n",
       "ip_205                  473122\n",
       "ip_207                  473122\n",
       "ip_8                    473122\n",
       "not_freq                473122\n",
       "ip_4                    473122\n",
       "ip_11                   473122\n",
       "dest_ip_130_1_8         473122\n",
       "not_dest_freq           473122\n",
       "Wrong_port              473122\n",
       "port_is_443             473122\n",
       "Wrong_port.1            473122\n",
       "destination_port_443    473122\n",
       "Duration                473122\n",
       "fs_1807                 473122\n",
       "fs_more                 473122\n",
       "fsless                  473122\n",
       "FlowSentRate            473122\n",
       "FlowRecvRate            473122\n",
       "Pcket_lenVar            473122\n",
       "Length_std              473122\n",
       "Length_mean             473122\n",
       "L_median                473122\n",
       "L_mode                  473122\n",
       "s_median                473122\n",
       "s_mode                  473122\n",
       "PLCV                    473122\n",
       "PTV                     473122\n",
       "PT_std                  473122\n",
       "PT_mean                 473122\n",
       "PT_median               473122\n",
       "Packt_Time_mode         473122\n",
       "s_mode_time             473122\n",
       "0                       473122\n",
       "r_time_var              473122\n",
       "r_time_std              473122\n",
       "r_time_mean             473122\n",
       "r_time_med              473122\n",
       "r_time_mode             473122\n",
       "r_s_med_time            473122\n",
       "r_s_mode_time           473122\n",
       "r_c_time                473122\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bac08cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X_resampled,y_resampled,train_size=0.8,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3632db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "181ae1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378497, 44)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22293013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.6017 - accuracy: 0.7581\n",
      "Epoch 2/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.3318 - accuracy: 0.9181\n",
      "Epoch 3/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.1991 - accuracy: 0.9442\n",
      "Epoch 4/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.1519 - accuracy: 0.9549\n",
      "Epoch 5/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.1306 - accuracy: 0.9578\n",
      "Epoch 6/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.1181 - accuracy: 0.9595\n",
      "Epoch 7/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.1095 - accuracy: 0.9615\n",
      "Epoch 8/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.1016 - accuracy: 0.9641\n",
      "Epoch 9/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0959 - accuracy: 0.9662\n",
      "Epoch 10/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0913 - accuracy: 0.9681\n",
      "Epoch 11/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0872 - accuracy: 0.9696\n",
      "Epoch 12/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0826 - accuracy: 0.9715\n",
      "Epoch 13/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0802 - accuracy: 0.9726\n",
      "Epoch 14/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0772 - accuracy: 0.9739\n",
      "Epoch 15/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0750 - accuracy: 0.9747\n",
      "Epoch 16/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0731 - accuracy: 0.9752\n",
      "Epoch 17/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0710 - accuracy: 0.9760\n",
      "Epoch 18/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0695 - accuracy: 0.9765\n",
      "Epoch 19/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0684 - accuracy: 0.9768\n",
      "Epoch 20/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0669 - accuracy: 0.9774\n",
      "Epoch 21/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0660 - accuracy: 0.9774\n",
      "Epoch 22/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0650 - accuracy: 0.9777\n",
      "Epoch 23/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0638 - accuracy: 0.9777\n",
      "Epoch 24/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0631 - accuracy: 0.9782\n",
      "Epoch 25/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0622 - accuracy: 0.9781\n",
      "Epoch 26/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0619 - accuracy: 0.9784\n",
      "Epoch 27/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0613 - accuracy: 0.9786\n",
      "Epoch 28/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0608 - accuracy: 0.9787\n",
      "Epoch 29/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0601 - accuracy: 0.9788\n",
      "Epoch 30/50\n",
      "2957/2957 [==============================] - 14s 5ms/step - loss: 0.0600 - accuracy: 0.9787\n",
      "Epoch 31/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0592 - accuracy: 0.9792\n",
      "Epoch 32/50\n",
      "2957/2957 [==============================] - 13s 4ms/step - loss: 0.0586 - accuracy: 0.9791\n",
      "Epoch 33/50\n",
      "2957/2957 [==============================] - 12s 4ms/step - loss: 0.0581 - accuracy: 0.9797\n",
      "Epoch 34/50\n",
      "2957/2957 [==============================] - 12s 4ms/step - loss: 0.0576 - accuracy: 0.9798\n",
      "Epoch 35/50\n",
      "2957/2957 [==============================] - 12s 4ms/step - loss: 0.0578 - accuracy: 0.9798\n",
      "Epoch 36/50\n",
      "2957/2957 [==============================] - 12s 4ms/step - loss: 0.0574 - accuracy: 0.9799\n",
      "Epoch 37/50\n",
      "2957/2957 [==============================] - 12s 4ms/step - loss: 0.0573 - accuracy: 0.9800\n",
      "Epoch 38/50\n",
      "2957/2957 [==============================] - 12s 4ms/step - loss: 0.0569 - accuracy: 0.9799\n",
      "Epoch 39/50\n",
      "2957/2957 [==============================] - 12s 4ms/step - loss: 0.0564 - accuracy: 0.9801\n",
      "Epoch 40/50\n",
      "2957/2957 [==============================] - 12s 4ms/step - loss: 0.0562 - accuracy: 0.9806\n",
      "Epoch 41/50\n",
      "2957/2957 [==============================] - 12s 4ms/step - loss: 0.0561 - accuracy: 0.9804\n",
      "Epoch 42/50\n",
      "2957/2957 [==============================] - 13s 4ms/step - loss: 0.0557 - accuracy: 0.9806\n",
      "Epoch 43/50\n",
      "2957/2957 [==============================] - 13s 4ms/step - loss: 0.0554 - accuracy: 0.9809\n",
      "Epoch 44/50\n",
      "2957/2957 [==============================] - 12s 4ms/step - loss: 0.0555 - accuracy: 0.9807\n",
      "Epoch 45/50\n",
      "2957/2957 [==============================] - 14s 5ms/step - loss: 0.0551 - accuracy: 0.9809\n",
      "Epoch 46/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0550 - accuracy: 0.9807\n",
      "Epoch 47/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0545 - accuracy: 0.9811\n",
      "Epoch 48/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0547 - accuracy: 0.9809\n",
      "Epoch 49/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0545 - accuracy: 0.9812\n",
      "Epoch 50/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0543 - accuracy: 0.9813\n",
      "5915/5915 [==============================] - 13s 2ms/step - loss: 0.0509 - accuracy: 0.9825\n",
      "0th Fold :\n",
      "accuracy: 98.25%\n",
      "------------------------------------------------------------------\n",
      "Epoch 1/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.5430 - accuracy: 0.8155\n",
      "Epoch 2/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.2679 - accuracy: 0.9255\n",
      "Epoch 3/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.1738 - accuracy: 0.9469\n",
      "Epoch 4/50\n",
      "2958/2958 [==============================] - 16s 5ms/step - loss: 0.1383 - accuracy: 0.9568\n",
      "Epoch 5/50\n",
      "2958/2958 [==============================] - 16s 5ms/step - loss: 0.1206 - accuracy: 0.9598\n",
      "Epoch 6/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.1106 - accuracy: 0.9619\n",
      "Epoch 7/50\n",
      "2958/2958 [==============================] - 16s 5ms/step - loss: 0.1021 - accuracy: 0.9642\n",
      "Epoch 8/50\n",
      "2958/2958 [==============================] - 15s 5ms/step - loss: 0.0968 - accuracy: 0.9655\n",
      "Epoch 9/50\n",
      "2958/2958 [==============================] - 15s 5ms/step - loss: 0.0910 - accuracy: 0.9679\n",
      "Epoch 10/50\n",
      "2958/2958 [==============================] - 1140s 386ms/step - loss: 0.0870 - accuracy: 0.9697\n",
      "Epoch 11/50\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0829 - accuracy: 0.9712\n",
      "Epoch 12/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0796 - accuracy: 0.9724\n",
      "Epoch 13/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0776 - accuracy: 0.9738\n",
      "Epoch 14/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0753 - accuracy: 0.9748\n",
      "Epoch 15/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0731 - accuracy: 0.9756\n",
      "Epoch 16/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0715 - accuracy: 0.9759\n",
      "Epoch 17/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0702 - accuracy: 0.9762\n",
      "Epoch 18/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0683 - accuracy: 0.9768\n",
      "Epoch 19/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0670 - accuracy: 0.9772\n",
      "Epoch 20/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0661 - accuracy: 0.9775\n",
      "Epoch 21/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0651 - accuracy: 0.9779\n",
      "Epoch 22/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0641 - accuracy: 0.9779\n",
      "Epoch 23/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0632 - accuracy: 0.9780\n",
      "Epoch 24/50\n",
      "2958/2958 [==============================] - 16s 6ms/step - loss: 0.0626 - accuracy: 0.9783\n",
      "Epoch 25/50\n",
      "2958/2958 [==============================] - 16s 6ms/step - loss: 0.0618 - accuracy: 0.9786\n",
      "Epoch 26/50\n",
      "2958/2958 [==============================] - 16s 6ms/step - loss: 0.0611 - accuracy: 0.9786\n",
      "Epoch 27/50\n",
      "2958/2958 [==============================] - 16s 6ms/step - loss: 0.0608 - accuracy: 0.9787\n",
      "Epoch 28/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0606 - accuracy: 0.9788\n",
      "Epoch 29/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0597 - accuracy: 0.9792\n",
      "Epoch 30/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0592 - accuracy: 0.9794\n",
      "Epoch 31/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0589 - accuracy: 0.9795\n",
      "Epoch 32/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0586 - accuracy: 0.9794\n",
      "Epoch 33/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0582 - accuracy: 0.9797\n",
      "Epoch 34/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0577 - accuracy: 0.9799\n",
      "Epoch 35/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0577 - accuracy: 0.9800\n",
      "Epoch 36/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0569 - accuracy: 0.9803\n",
      "Epoch 37/50\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0569 - accuracy: 0.9801\n",
      "Epoch 38/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0567 - accuracy: 0.9803\n",
      "Epoch 39/50\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0563 - accuracy: 0.9806\n",
      "Epoch 40/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0561 - accuracy: 0.9806\n",
      "Epoch 41/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0558 - accuracy: 0.9805\n",
      "Epoch 42/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0556 - accuracy: 0.9808\n",
      "Epoch 43/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0552 - accuracy: 0.9811\n",
      "Epoch 44/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0554 - accuracy: 0.9809\n",
      "Epoch 45/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0552 - accuracy: 0.9806\n",
      "Epoch 46/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0549 - accuracy: 0.9810\n",
      "Epoch 47/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0544 - accuracy: 0.9812\n",
      "Epoch 48/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0548 - accuracy: 0.9809\n",
      "Epoch 49/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0543 - accuracy: 0.9812\n",
      "Epoch 50/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0544 - accuracy: 0.9813\n",
      "5914/5914 [==============================] - 16s 3ms/step - loss: 0.0507 - accuracy: 0.9824\n",
      "1th Fold :\n",
      "accuracy: 98.24%\n",
      "------------------------------------------------------------------\n",
      "Average validation accuracy : \n",
      "98.25% (+/- 0.00%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "i=0\n",
    "for train, test in kfold.split(x_train, y_train):\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Convolution1D(64, 3, padding=\"same\",activation=\"relu\",input_shape=(44,1)))\n",
    "    cnn.add(MaxPooling1D(pool_size=(2)))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation=\"relu\"))\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(Dense(2, activation=\"sigmoid\"))\n",
    "    # define optimizer and objective, compile cnn\n",
    "\n",
    "\n",
    "    cnn.compile(loss=\"binary_crossentropy\", optimizer=\"Adagrad\",metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    x_tn = x_train.iloc[train]\n",
    "    y_tn = y_train.iloc[train]\n",
    "    y_tn = pd.get_dummies(y_tn)\n",
    "    x_ts = x_train.iloc[test]\n",
    "    y_ts = y_train.iloc[test]\n",
    "    y_ts = pd.get_dummies(y_ts)\n",
    "    \n",
    "    x_tn1 = x_tn.to_numpy()\n",
    "    x_tn1 = np.reshape(x_tn1, (x_tn1.shape[0],x_tn1.shape[1],1))\n",
    "    \n",
    "    \n",
    "    x_ts1 = x_ts.to_numpy()\n",
    "    x_ts1 = np.reshape(x_ts1, (x_ts1.shape[0],x_ts1.shape[1],1))\n",
    "    \n",
    "    cnn.fit(x_tn1, y_tn, epochs=50,batch_size=64,verbose=1)\n",
    "    scores = cnn.evaluate(x_ts1, y_ts, verbose=1)\n",
    "    print(str(i)+\"th Fold :\")\n",
    "    print(\"%s: %.2f%%\" % (cnn.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    i = i+1\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Average validation accuracy : \")   \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c44e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x_train.to_numpy()\n",
    "x_tr = np.reshape(x_tr, (x_tr.shape[0], x_tr.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fb8a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts = x_test.to_numpy()\n",
    "x_ts = np.reshape(x_ts, (x_ts.shape[0], x_ts.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc1655d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = pd.get_dummies(y_train)\n",
    "y_ts = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a70cc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11829/11829 [==============================] - 34s 3ms/step - loss: 0.0507 - accuracy: 0.9825\n",
      "2958/2958 [==============================] - 8s 3ms/step - loss: 0.0519 - accuracy: 0.9819\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = cnn.evaluate(x_tr, y_tr, verbose=1)\n",
    "_, test_acc = cnn.evaluate(x_ts, y_ts, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3314377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.2515037059784\n",
      "Test accuracy: 98.19498062133789\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: \"+str(train_acc*100))\n",
    "print(\"Test accuracy: \"+str(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0af3d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2958/2958 [==============================] - 7s 2ms/step\n",
      "2958/2958 [==============================] - 7s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_probs = cnn.predict(x_ts, verbose=1).ravel()\n",
    "#y_classes = cnn.predict_classes(x_ts, verbose=1)\n",
    "#y_classes = (cnn.predict(x_ts) > 0.5).astype(\"int32\")\n",
    "y_classes = np.argmax(cnn.predict(x_ts), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f02bba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.get_dummies(y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abf6cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     48236\n",
      "           1       0.98      0.98      0.98     46389\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     94625\n",
      "   macro avg       0.98      0.98      0.98     94625\n",
      "weighted avg       0.98      0.98      0.98     94625\n",
      " samples avg       0.98      0.98      0.98     94625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_ts,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d522cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feb8a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_le = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d067f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test_le, y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82ae788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e226d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1db9248b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9819090794925767\n"
     ]
    }
   ],
   "source": [
    "print(auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84e6591b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(fpr_keras, tpr_keras)\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58c369d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0714 - accuracy: 0.9751\n",
      "Epoch 2/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0545 - accuracy: 0.9809\n",
      "Epoch 3/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0525 - accuracy: 0.9818\n",
      "Epoch 4/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0508 - accuracy: 0.9818\n",
      "Epoch 5/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0500 - accuracy: 0.9821\n",
      "Epoch 6/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0491 - accuracy: 0.9825\n",
      "Epoch 7/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0487 - accuracy: 0.9825\n",
      "Epoch 8/50\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0478 - accuracy: 0.9829\n",
      "Epoch 9/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0478 - accuracy: 0.9830\n",
      "Epoch 10/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0474 - accuracy: 0.9829\n",
      "Epoch 11/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0472 - accuracy: 0.9833\n",
      "Epoch 12/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0469 - accuracy: 0.9832\n",
      "Epoch 13/50\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0470 - accuracy: 0.9835\n",
      "Epoch 14/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0463 - accuracy: 0.9836\n",
      "Epoch 15/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0464 - accuracy: 0.9834\n",
      "Epoch 16/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0461 - accuracy: 0.9837\n",
      "Epoch 17/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0462 - accuracy: 0.9836\n",
      "Epoch 18/50\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0457 - accuracy: 0.9838\n",
      "Epoch 19/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0458 - accuracy: 0.9837\n",
      "Epoch 20/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0454 - accuracy: 0.9841\n",
      "Epoch 21/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0454 - accuracy: 0.9838\n",
      "Epoch 22/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0454 - accuracy: 0.9839\n",
      "Epoch 23/50\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0453 - accuracy: 0.9839\n",
      "Epoch 24/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0451 - accuracy: 0.9841\n",
      "Epoch 25/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0450 - accuracy: 0.9841\n",
      "Epoch 26/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0445 - accuracy: 0.9840\n",
      "Epoch 27/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0447 - accuracy: 0.9842\n",
      "Epoch 28/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0446 - accuracy: 0.9842\n",
      "Epoch 29/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0449 - accuracy: 0.9843\n",
      "Epoch 30/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0444 - accuracy: 0.9844\n",
      "Epoch 31/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0444 - accuracy: 0.9843\n",
      "Epoch 32/50\n",
      "2957/2957 [==============================] - 20s 7ms/step - loss: 0.0446 - accuracy: 0.9841\n",
      "Epoch 33/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0443 - accuracy: 0.9845\n",
      "Epoch 34/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0441 - accuracy: 0.9845\n",
      "Epoch 35/50\n",
      "2957/2957 [==============================] - 19s 7ms/step - loss: 0.0445 - accuracy: 0.9846\n",
      "Epoch 36/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0441 - accuracy: 0.9844\n",
      "Epoch 37/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0444 - accuracy: 0.9846\n",
      "Epoch 38/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0441 - accuracy: 0.9844\n",
      "Epoch 39/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0441 - accuracy: 0.9845\n",
      "Epoch 40/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0440 - accuracy: 0.9846\n",
      "Epoch 41/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0436 - accuracy: 0.9846\n",
      "Epoch 42/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0440 - accuracy: 0.9847\n",
      "Epoch 43/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0435 - accuracy: 0.9848\n",
      "Epoch 44/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0440 - accuracy: 0.9850\n",
      "Epoch 45/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0438 - accuracy: 0.9845\n",
      "Epoch 46/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0438 - accuracy: 0.9847\n",
      "Epoch 47/50\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0438 - accuracy: 0.9846\n",
      "Epoch 48/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0438 - accuracy: 0.9848\n",
      "Epoch 49/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0434 - accuracy: 0.9849\n",
      "Epoch 50/50\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0439 - accuracy: 0.9846\n",
      "5915/5915 [==============================] - 17s 3ms/step - loss: 0.0418 - accuracy: 0.9851\n",
      "0th Fold :\n",
      "accuracy: 98.51%\n",
      "------------------------------------------------------------------\n",
      "Epoch 1/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0708 - accuracy: 0.9750\n",
      "Epoch 2/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0541 - accuracy: 0.9809\n",
      "Epoch 3/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0526 - accuracy: 0.9813\n",
      "Epoch 4/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0507 - accuracy: 0.9817\n",
      "Epoch 5/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0501 - accuracy: 0.9821\n",
      "Epoch 6/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0491 - accuracy: 0.9824\n",
      "Epoch 7/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0486 - accuracy: 0.9825\n",
      "Epoch 8/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0484 - accuracy: 0.9828\n",
      "Epoch 9/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0480 - accuracy: 0.9830\n",
      "Epoch 10/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0474 - accuracy: 0.9831\n",
      "Epoch 11/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0472 - accuracy: 0.9831\n",
      "Epoch 12/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0471 - accuracy: 0.9835\n",
      "Epoch 13/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0469 - accuracy: 0.9834\n",
      "Epoch 14/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0463 - accuracy: 0.9835\n",
      "Epoch 15/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0465 - accuracy: 0.9835\n",
      "Epoch 16/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0466 - accuracy: 0.9833\n",
      "Epoch 17/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0460 - accuracy: 0.9834\n",
      "Epoch 18/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0461 - accuracy: 0.9835\n",
      "Epoch 19/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0459 - accuracy: 0.9838\n",
      "Epoch 20/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0460 - accuracy: 0.9838\n",
      "Epoch 21/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0455 - accuracy: 0.9838\n",
      "Epoch 22/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0458 - accuracy: 0.9839\n",
      "Epoch 23/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0452 - accuracy: 0.9840\n",
      "Epoch 24/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0456 - accuracy: 0.9841\n",
      "Epoch 25/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0451 - accuracy: 0.9842\n",
      "Epoch 26/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0450 - accuracy: 0.9841\n",
      "Epoch 27/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0451 - accuracy: 0.9839\n",
      "Epoch 28/50\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0450 - accuracy: 0.9842\n",
      "Epoch 29/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0449 - accuracy: 0.9845\n",
      "Epoch 30/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0450 - accuracy: 0.9839\n",
      "Epoch 31/50\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0448 - accuracy: 0.9839\n",
      "Epoch 32/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0445 - accuracy: 0.9842\n",
      "Epoch 33/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0446 - accuracy: 0.9845\n",
      "Epoch 34/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0447 - accuracy: 0.9843\n",
      "Epoch 35/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0443 - accuracy: 0.9844\n",
      "Epoch 36/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0441 - accuracy: 0.9842\n",
      "Epoch 37/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0444 - accuracy: 0.9843\n",
      "Epoch 38/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0443 - accuracy: 0.9841\n",
      "Epoch 39/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0443 - accuracy: 0.9842\n",
      "Epoch 40/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0441 - accuracy: 0.9845\n",
      "Epoch 41/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0443 - accuracy: 0.9844\n",
      "Epoch 42/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0441 - accuracy: 0.9843\n",
      "Epoch 43/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0441 - accuracy: 0.9845\n",
      "Epoch 44/50\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0438 - accuracy: 0.9845\n",
      "Epoch 45/50\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0437 - accuracy: 0.9845\n",
      "Epoch 46/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0438 - accuracy: 0.9847\n",
      "Epoch 47/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0439 - accuracy: 0.9846\n",
      "Epoch 48/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0439 - accuracy: 0.9844\n",
      "Epoch 49/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0437 - accuracy: 0.9847\n",
      "Epoch 50/50\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0436 - accuracy: 0.9846\n",
      "5914/5914 [==============================] - 15s 3ms/step - loss: 0.0426 - accuracy: 0.9844\n",
      "1th Fold :\n",
      "accuracy: 98.44%\n",
      "------------------------------------------------------------------\n",
      "Average validation accuracy : \n",
      "98.48% (+/- 0.03%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "i=0\n",
    "for train, test in kfold.split(x_train, y_train):\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Convolution1D(64, 3, padding=\"same\",activation=\"relu\",input_shape=(44,1)))\n",
    "    cnn.add(MaxPooling1D(pool_size=(2)))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation=\"relu\"))\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(Dense(2, activation=\"sigmoid\"))\n",
    "    # define optimizer and objective, compile cnn\n",
    "\n",
    "\n",
    "    cnn.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\",metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    x_tn = x_train.iloc[train]\n",
    "    y_tn = y_train.iloc[train]\n",
    "    y_tn = pd.get_dummies(y_tn)\n",
    "    x_ts = x_train.iloc[test]\n",
    "    y_ts = y_train.iloc[test]\n",
    "    y_ts = pd.get_dummies(y_ts)\n",
    "    \n",
    "    x_tn1 = x_tn.to_numpy()\n",
    "    x_tn1 = np.reshape(x_tn1, (x_tn1.shape[0],x_tn1.shape[1],1))\n",
    "    \n",
    "    \n",
    "    x_ts1 = x_ts.to_numpy()\n",
    "    x_ts1 = np.reshape(x_ts1, (x_ts1.shape[0],x_ts1.shape[1],1))\n",
    "    \n",
    "    cnn.fit(x_tn1, y_tn, epochs=50,batch_size=64,verbose=1)\n",
    "    scores = cnn.evaluate(x_ts1, y_ts, verbose=1)\n",
    "    print(str(i)+\"th Fold :\")\n",
    "    print(\"%s: %.2f%%\" % (cnn.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    i = i+1\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Average validation accuracy : \")   \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51353a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x_train.to_numpy()\n",
    "x_tr = np.reshape(x_tr, (x_tr.shape[0], x_tr.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bfee000",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts = x_test.to_numpy()\n",
    "x_ts = np.reshape(x_ts, (x_ts.shape[0], x_ts.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32338020",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = pd.get_dummies(y_train)\n",
    "y_ts = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35289812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11829/11829 [==============================] - 30s 2ms/step - loss: 0.0418 - accuracy: 0.9846\n",
      "2958/2958 [==============================] - 7s 2ms/step - loss: 0.0427 - accuracy: 0.9840\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = cnn.evaluate(x_tr, y_tr, verbose=1)\n",
    "_, test_acc = cnn.evaluate(x_ts, y_ts, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f09b87d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.46366047859192\n",
      "Test accuracy: 98.4000027179718\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: \"+str(train_acc*100))\n",
    "print(\"Test accuracy: \"+str(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22bfc538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2958/2958 [==============================] - 6s 2ms/step\n",
      "2958/2958 [==============================] - 6s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_probs = cnn.predict(x_ts, verbose=1).ravel()\n",
    "#y_classes = cnn.predict_classes(x_ts, verbose=1)\n",
    "#y_classes = (cnn.predict(x_ts) > 0.5).astype(\"int32\")\n",
    "y_classes = np.argmax(cnn.predict(x_ts), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03d4f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.get_dummies(y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69554a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     48236\n",
      "           1       0.98      0.98      0.98     46389\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     94625\n",
      "   macro avg       0.98      0.98      0.98     94625\n",
      "weighted avg       0.98      0.98      0.98     94625\n",
      " samples avg       0.98      0.98      0.98     94625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_ts,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f75e13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0b916be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_le = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98a38c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test_le, y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ae84ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b41bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69a85132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9839996797329025\n"
     ]
    }
   ],
   "source": [
    "print(auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e6fd076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(fpr_keras, tpr_keras)\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dad807c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2957/2957 [==============================] - 19s 6ms/step - loss: 0.0678 - accuracy: 0.9764\n",
      "Epoch 2/50\n",
      "2957/2957 [==============================] - 19s 7ms/step - loss: 0.0544 - accuracy: 0.9810\n",
      "Epoch 3/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0529 - accuracy: 0.9815\n",
      "Epoch 4/50\n",
      "2957/2957 [==============================] - 19s 6ms/step - loss: 0.0522 - accuracy: 0.9817\n",
      "Epoch 5/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0517 - accuracy: 0.9821\n",
      "Epoch 6/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0521 - accuracy: 0.9815\n",
      "Epoch 7/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0519 - accuracy: 0.9817\n",
      "Epoch 8/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0519 - accuracy: 0.9822\n",
      "Epoch 9/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0522 - accuracy: 0.9819\n",
      "Epoch 10/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0523 - accuracy: 0.9820\n",
      "Epoch 11/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0524 - accuracy: 0.9819\n",
      "Epoch 12/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0527 - accuracy: 0.9819\n",
      "Epoch 13/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0527 - accuracy: 0.9818\n",
      "Epoch 14/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0533 - accuracy: 0.9817\n",
      "Epoch 15/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0541 - accuracy: 0.9819\n",
      "Epoch 16/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0542 - accuracy: 0.9815\n",
      "Epoch 17/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0537 - accuracy: 0.9817\n",
      "Epoch 18/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0530 - accuracy: 0.9819\n",
      "Epoch 19/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0540 - accuracy: 0.9816\n",
      "Epoch 20/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0541 - accuracy: 0.9817\n",
      "Epoch 21/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0540 - accuracy: 0.9817\n",
      "Epoch 22/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0541 - accuracy: 0.9819\n",
      "Epoch 23/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0544 - accuracy: 0.9819\n",
      "Epoch 24/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0547 - accuracy: 0.9815\n",
      "Epoch 25/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0540 - accuracy: 0.9817\n",
      "Epoch 26/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0545 - accuracy: 0.9815\n",
      "Epoch 27/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0543 - accuracy: 0.9816\n",
      "Epoch 28/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0549 - accuracy: 0.9815\n",
      "Epoch 29/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0561 - accuracy: 0.9815\n",
      "Epoch 30/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0554 - accuracy: 0.9813\n",
      "Epoch 31/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0569 - accuracy: 0.9814\n",
      "Epoch 32/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0565 - accuracy: 0.9809\n",
      "Epoch 33/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0569 - accuracy: 0.9811\n",
      "Epoch 34/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0588 - accuracy: 0.9808\n",
      "Epoch 35/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0571 - accuracy: 0.9809\n",
      "Epoch 36/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0579 - accuracy: 0.9811\n",
      "Epoch 37/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0581 - accuracy: 0.9809\n",
      "Epoch 38/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0578 - accuracy: 0.9808\n",
      "Epoch 39/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0577 - accuracy: 0.9806\n",
      "Epoch 40/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0574 - accuracy: 0.9809\n",
      "Epoch 41/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0584 - accuracy: 0.9805\n",
      "Epoch 42/50\n",
      "2957/2957 [==============================] - 22s 7ms/step - loss: 0.0575 - accuracy: 0.9805\n",
      "Epoch 43/50\n",
      "2957/2957 [==============================] - 19s 6ms/step - loss: 0.0582 - accuracy: 0.9809\n",
      "Epoch 44/50\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0576 - accuracy: 0.9807\n",
      "Epoch 45/50\n",
      "2957/2957 [==============================] - 20s 7ms/step - loss: 0.0584 - accuracy: 0.9807\n",
      "Epoch 46/50\n",
      "2957/2957 [==============================] - 21s 7ms/step - loss: 0.0584 - accuracy: 0.9811\n",
      "Epoch 47/50\n",
      "2957/2957 [==============================] - 22s 7ms/step - loss: 0.0583 - accuracy: 0.9810\n",
      "Epoch 48/50\n",
      "2957/2957 [==============================] - 21s 7ms/step - loss: 0.0583 - accuracy: 0.9810\n",
      "Epoch 49/50\n",
      "2957/2957 [==============================] - 19s 6ms/step - loss: 0.0592 - accuracy: 0.9808\n",
      "Epoch 50/50\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0582 - accuracy: 0.9802\n",
      "5915/5915 [==============================] - 15s 2ms/step - loss: 0.0560 - accuracy: 0.9817\n",
      "0th Fold :\n",
      "accuracy: 98.17%\n",
      "------------------------------------------------------------------\n",
      "Epoch 1/50\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0678 - accuracy: 0.9759\n",
      "Epoch 2/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0539 - accuracy: 0.9812\n",
      "Epoch 3/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0525 - accuracy: 0.9814\n",
      "Epoch 4/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0523 - accuracy: 0.9817\n",
      "Epoch 5/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0522 - accuracy: 0.9817\n",
      "Epoch 6/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0527 - accuracy: 0.9817\n",
      "Epoch 7/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0526 - accuracy: 0.9818\n",
      "Epoch 8/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0535 - accuracy: 0.9815\n",
      "Epoch 9/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0533 - accuracy: 0.9816\n",
      "Epoch 10/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0530 - accuracy: 0.9821\n",
      "Epoch 11/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0532 - accuracy: 0.9822\n",
      "Epoch 12/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0540 - accuracy: 0.9819\n",
      "Epoch 13/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0546 - accuracy: 0.9818\n",
      "Epoch 14/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0535 - accuracy: 0.9814\n",
      "Epoch 15/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0532 - accuracy: 0.9818\n",
      "Epoch 16/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0528 - accuracy: 0.9816\n",
      "Epoch 17/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0528 - accuracy: 0.9817\n",
      "Epoch 18/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0536 - accuracy: 0.9818\n",
      "Epoch 19/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0536 - accuracy: 0.9819\n",
      "Epoch 20/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0543 - accuracy: 0.9819\n",
      "Epoch 21/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0538 - accuracy: 0.9821\n",
      "Epoch 22/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0536 - accuracy: 0.9822\n",
      "Epoch 23/50\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0549 - accuracy: 0.9817\n",
      "Epoch 24/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0539 - accuracy: 0.9818\n",
      "Epoch 25/50\n",
      "2958/2958 [==============================] - 23s 8ms/step - loss: 0.0535 - accuracy: 0.9817\n",
      "Epoch 26/50\n",
      "2958/2958 [==============================] - 24s 8ms/step - loss: 0.0541 - accuracy: 0.9818\n",
      "Epoch 27/50\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0539 - accuracy: 0.9820\n",
      "Epoch 28/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0536 - accuracy: 0.9816\n",
      "Epoch 29/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0545 - accuracy: 0.9815\n",
      "Epoch 30/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0546 - accuracy: 0.9816\n",
      "Epoch 31/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0548 - accuracy: 0.9813\n",
      "Epoch 32/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0555 - accuracy: 0.9814\n",
      "Epoch 33/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0566 - accuracy: 0.9814\n",
      "Epoch 34/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0560 - accuracy: 0.9813\n",
      "Epoch 35/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0559 - accuracy: 0.9814\n",
      "Epoch 36/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0565 - accuracy: 0.9812\n",
      "Epoch 37/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0567 - accuracy: 0.9813\n",
      "Epoch 38/50\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0562 - accuracy: 0.9808\n",
      "Epoch 39/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0580 - accuracy: 0.9808\n",
      "Epoch 40/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0572 - accuracy: 0.9808\n",
      "Epoch 41/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0582 - accuracy: 0.9808\n",
      "Epoch 42/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0574 - accuracy: 0.9808\n",
      "Epoch 43/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0567 - accuracy: 0.9805\n",
      "Epoch 44/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0580 - accuracy: 0.9808\n",
      "Epoch 45/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0583 - accuracy: 0.9808\n",
      "Epoch 46/50\n",
      "2958/2958 [==============================] - 23s 8ms/step - loss: 0.0583 - accuracy: 0.9808\n",
      "Epoch 47/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0587 - accuracy: 0.9804\n",
      "Epoch 48/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0592 - accuracy: 0.9810\n",
      "Epoch 49/50\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0588 - accuracy: 0.9808\n",
      "Epoch 50/50\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0578 - accuracy: 0.9809\n",
      "5914/5914 [==============================] - 16s 3ms/step - loss: 0.0542 - accuracy: 0.9822\n",
      "1th Fold :\n",
      "accuracy: 98.22%\n",
      "------------------------------------------------------------------\n",
      "Average validation accuracy : \n",
      "98.19% (+/- 0.02%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "i=0\n",
    "for train, test in kfold.split(x_train, y_train):\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Convolution1D(64, 3, padding=\"same\",activation=\"relu\",input_shape=(44,1)))\n",
    "    cnn.add(MaxPooling1D(pool_size=(2)))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation=\"relu\"))\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(Dense(2, activation=\"sigmoid\"))\n",
    "    # define optimizer and objective, compile cnn\n",
    "\n",
    "\n",
    "    cnn.compile(loss=\"binary_crossentropy\", optimizer=\"RMSProp\",metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    x_tn = x_train.iloc[train]\n",
    "    y_tn = y_train.iloc[train]\n",
    "    y_tn = pd.get_dummies(y_tn)\n",
    "    x_ts = x_train.iloc[test]\n",
    "    y_ts = y_train.iloc[test]\n",
    "    y_ts = pd.get_dummies(y_ts)\n",
    "    \n",
    "    x_tn1 = x_tn.to_numpy()\n",
    "    x_tn1 = np.reshape(x_tn1, (x_tn1.shape[0],x_tn1.shape[1],1))\n",
    "    \n",
    "    \n",
    "    x_ts1 = x_ts.to_numpy()\n",
    "    x_ts1 = np.reshape(x_ts1, (x_ts1.shape[0],x_ts1.shape[1],1))\n",
    "    \n",
    "    cnn.fit(x_tn1, y_tn, epochs=50,batch_size=64,verbose=1)\n",
    "    scores = cnn.evaluate(x_ts1, y_ts, verbose=1)\n",
    "    print(str(i)+\"th Fold :\")\n",
    "    print(\"%s: %.2f%%\" % (cnn.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    i = i+1\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Average validation accuracy : \")   \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73d33763",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x_train.to_numpy()\n",
    "x_tr = np.reshape(x_tr, (x_tr.shape[0], x_tr.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "368cdab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts = x_test.to_numpy()\n",
    "x_ts = np.reshape(x_ts, (x_ts.shape[0], x_ts.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a54bdf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = pd.get_dummies(y_train)\n",
    "y_ts = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ca0fc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11829/11829 [==============================] - 32s 3ms/step - loss: 0.0541 - accuracy: 0.9821\n",
      "2958/2958 [==============================] - 8s 3ms/step - loss: 0.0550 - accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = cnn.evaluate(x_tr, y_tr, verbose=1)\n",
    "_, test_acc = cnn.evaluate(x_ts, y_ts, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "893da1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.2094943523407\n",
      "Test accuracy: 98.18229675292969\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: \"+str(train_acc*100))\n",
    "print(\"Test accuracy: \"+str(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10e9508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2958/2958 [==============================] - 7s 2ms/step\n",
      "2958/2958 [==============================] - 7s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_probs = cnn.predict(x_ts, verbose=1).ravel()\n",
    "#y_classes = cnn.predict_classes(x_ts, verbose=1)\n",
    "#y_classes = (cnn.predict(x_ts) > 0.5).astype(\"int32\")\n",
    "y_classes = np.argmax(cnn.predict(x_ts), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88174a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.get_dummies(y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a1a005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     48236\n",
      "           1       0.98      0.98      0.98     46389\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     94625\n",
      "   macro avg       0.98      0.98      0.98     94625\n",
      "weighted avg       0.98      0.98      0.98     94625\n",
      " samples avg       0.98      0.98      0.98     94625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_ts,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0e93caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0c8d11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_le = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f065ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test_le, y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6499e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb5bed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9066a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9818659959893705\n"
     ]
    }
   ],
   "source": [
    "print(auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e64a511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(fpr_keras, tpr_keras)\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83587ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2957/2957 [==============================] - 19s 6ms/step - loss: 0.0712 - accuracy: 0.9749\n",
      "Epoch 2/100\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0544 - accuracy: 0.9808\n",
      "Epoch 3/100\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0522 - accuracy: 0.9810\n",
      "Epoch 4/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0509 - accuracy: 0.9819\n",
      "Epoch 5/100\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0496 - accuracy: 0.9821\n",
      "Epoch 6/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0494 - accuracy: 0.9823\n",
      "Epoch 7/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0482 - accuracy: 0.9827\n",
      "Epoch 8/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0480 - accuracy: 0.9827\n",
      "Epoch 9/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0478 - accuracy: 0.9830\n",
      "Epoch 10/100\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0474 - accuracy: 0.9833\n",
      "Epoch 11/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0470 - accuracy: 0.9835\n",
      "Epoch 12/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0470 - accuracy: 0.9835\n",
      "Epoch 13/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0469 - accuracy: 0.9832\n",
      "Epoch 14/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0461 - accuracy: 0.9837\n",
      "Epoch 15/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0463 - accuracy: 0.9837\n",
      "Epoch 16/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0461 - accuracy: 0.9835\n",
      "Epoch 17/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0461 - accuracy: 0.9837\n",
      "Epoch 18/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0458 - accuracy: 0.9838\n",
      "Epoch 19/100\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0455 - accuracy: 0.9839\n",
      "Epoch 20/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0453 - accuracy: 0.9839\n",
      "Epoch 21/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0457 - accuracy: 0.9837\n",
      "Epoch 22/100\n",
      "2957/2957 [==============================] - 19s 6ms/step - loss: 0.0451 - accuracy: 0.9839\n",
      "Epoch 23/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0448 - accuracy: 0.9840\n",
      "Epoch 24/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0450 - accuracy: 0.9841\n",
      "Epoch 25/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0452 - accuracy: 0.9841\n",
      "Epoch 26/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0449 - accuracy: 0.9841\n",
      "Epoch 27/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0447 - accuracy: 0.9842\n",
      "Epoch 28/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0447 - accuracy: 0.9843\n",
      "Epoch 29/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0448 - accuracy: 0.9843\n",
      "Epoch 30/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0445 - accuracy: 0.9844\n",
      "Epoch 31/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0444 - accuracy: 0.9844\n",
      "Epoch 32/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0443 - accuracy: 0.9842\n",
      "Epoch 33/100\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0441 - accuracy: 0.9844\n",
      "Epoch 34/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0447 - accuracy: 0.9843\n",
      "Epoch 35/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0441 - accuracy: 0.9843\n",
      "Epoch 36/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0441 - accuracy: 0.9843\n",
      "Epoch 37/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0446 - accuracy: 0.9844\n",
      "Epoch 38/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0441 - accuracy: 0.9845\n",
      "Epoch 39/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0436 - accuracy: 0.9845\n",
      "Epoch 40/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0439 - accuracy: 0.9844\n",
      "Epoch 41/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0437 - accuracy: 0.9847\n",
      "Epoch 42/100\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0438 - accuracy: 0.9845\n",
      "Epoch 43/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0439 - accuracy: 0.9844\n",
      "Epoch 44/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0438 - accuracy: 0.9844\n",
      "Epoch 45/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0437 - accuracy: 0.9848\n",
      "Epoch 46/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0435 - accuracy: 0.9847\n",
      "Epoch 47/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0436 - accuracy: 0.9848\n",
      "Epoch 48/100\n",
      "2957/2957 [==============================] - 19s 6ms/step - loss: 0.0435 - accuracy: 0.9848\n",
      "Epoch 49/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0437 - accuracy: 0.9848\n",
      "Epoch 50/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0433 - accuracy: 0.9846\n",
      "Epoch 51/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0432 - accuracy: 0.9851\n",
      "Epoch 52/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0433 - accuracy: 0.9845\n",
      "Epoch 53/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0434 - accuracy: 0.9847\n",
      "Epoch 54/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0437 - accuracy: 0.9847\n",
      "Epoch 55/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0434 - accuracy: 0.9846\n",
      "Epoch 56/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0431 - accuracy: 0.9850\n",
      "Epoch 57/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0432 - accuracy: 0.9847\n",
      "Epoch 58/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0429 - accuracy: 0.9849\n",
      "Epoch 59/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0431 - accuracy: 0.9850\n",
      "Epoch 60/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0431 - accuracy: 0.9850\n",
      "Epoch 61/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0432 - accuracy: 0.9847\n",
      "Epoch 62/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0427 - accuracy: 0.9849\n",
      "Epoch 63/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0429 - accuracy: 0.9849\n",
      "Epoch 64/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0431 - accuracy: 0.9853\n",
      "Epoch 65/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0431 - accuracy: 0.9849\n",
      "Epoch 66/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0429 - accuracy: 0.9852\n",
      "Epoch 67/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0428 - accuracy: 0.9853\n",
      "Epoch 68/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0433 - accuracy: 0.9850\n",
      "Epoch 69/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0428 - accuracy: 0.9849\n",
      "Epoch 70/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0428 - accuracy: 0.9849\n",
      "Epoch 71/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0426 - accuracy: 0.9851\n",
      "Epoch 72/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0429 - accuracy: 0.9850\n",
      "Epoch 73/100\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0427 - accuracy: 0.9852\n",
      "Epoch 74/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0426 - accuracy: 0.9850\n",
      "Epoch 75/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0428 - accuracy: 0.9851\n",
      "Epoch 76/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0428 - accuracy: 0.9851\n",
      "Epoch 77/100\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0426 - accuracy: 0.9852\n",
      "Epoch 78/100\n",
      "2957/2957 [==============================] - 16s 6ms/step - loss: 0.0425 - accuracy: 0.9852\n",
      "Epoch 79/100\n",
      "2957/2957 [==============================] - 19s 7ms/step - loss: 0.0424 - accuracy: 0.9851\n",
      "Epoch 80/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0427 - accuracy: 0.9852\n",
      "Epoch 81/100\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0424 - accuracy: 0.9853\n",
      "Epoch 82/100\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0423 - accuracy: 0.9852\n",
      "Epoch 83/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0427 - accuracy: 0.9853\n",
      "Epoch 84/100\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0423 - accuracy: 0.9851\n",
      "Epoch 85/100\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0424 - accuracy: 0.9850\n",
      "Epoch 86/100\n",
      "2957/2957 [==============================] - 18s 6ms/step - loss: 0.0424 - accuracy: 0.9852\n",
      "Epoch 87/100\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0424 - accuracy: 0.9850\n",
      "Epoch 88/100\n",
      "2957/2957 [==============================] - 14s 5ms/step - loss: 0.0422 - accuracy: 0.9851\n",
      "Epoch 89/100\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0423 - accuracy: 0.9852\n",
      "Epoch 90/100\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0420 - accuracy: 0.9853\n",
      "Epoch 91/100\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0424 - accuracy: 0.9851\n",
      "Epoch 92/100\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0420 - accuracy: 0.9851\n",
      "Epoch 93/100\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0420 - accuracy: 0.9852\n",
      "Epoch 94/100\n",
      "2957/2957 [==============================] - 15s 5ms/step - loss: 0.0423 - accuracy: 0.9851\n",
      "Epoch 95/100\n",
      "2957/2957 [==============================] - 16s 5ms/step - loss: 0.0423 - accuracy: 0.9852\n",
      "Epoch 96/100\n",
      "2957/2957 [==============================] - 14s 5ms/step - loss: 0.0419 - accuracy: 0.9852\n",
      "Epoch 97/100\n",
      "2957/2957 [==============================] - 14s 5ms/step - loss: 0.0421 - accuracy: 0.9854\n",
      "Epoch 98/100\n",
      "2957/2957 [==============================] - 14s 5ms/step - loss: 0.0420 - accuracy: 0.9853\n",
      "Epoch 99/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0416 - accuracy: 0.9852\n",
      "Epoch 100/100\n",
      "2957/2957 [==============================] - 17s 6ms/step - loss: 0.0420 - accuracy: 0.9851\n",
      "5915/5915 [==============================] - 15s 2ms/step - loss: 0.0421 - accuracy: 0.9854\n",
      "0th Fold :\n",
      "accuracy: 98.54%\n",
      "------------------------------------------------------------------\n",
      "Epoch 1/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0712 - accuracy: 0.9752\n",
      "Epoch 2/100\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0546 - accuracy: 0.9806\n",
      "Epoch 3/100\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0521 - accuracy: 0.9815\n",
      "Epoch 4/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0508 - accuracy: 0.9818\n",
      "Epoch 5/100\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0499 - accuracy: 0.9820\n",
      "Epoch 6/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0499 - accuracy: 0.9821\n",
      "Epoch 7/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0490 - accuracy: 0.9825\n",
      "Epoch 8/100\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0485 - accuracy: 0.9828\n",
      "Epoch 9/100\n",
      "2958/2958 [==============================] - 678s 229ms/step - loss: 0.0482 - accuracy: 0.9828\n",
      "Epoch 10/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0479 - accuracy: 0.9831\n",
      "Epoch 11/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0476 - accuracy: 0.9833\n",
      "Epoch 12/100\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0473 - accuracy: 0.9831\n",
      "Epoch 13/100\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0468 - accuracy: 0.9835\n",
      "Epoch 14/100\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0468 - accuracy: 0.9836\n",
      "Epoch 15/100\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0467 - accuracy: 0.9836\n",
      "Epoch 16/100\n",
      "2958/2958 [==============================] - 23s 8ms/step - loss: 0.0467 - accuracy: 0.9836\n",
      "Epoch 17/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0461 - accuracy: 0.9839\n",
      "Epoch 18/100\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0463 - accuracy: 0.9837\n",
      "Epoch 19/100\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0459 - accuracy: 0.9837\n",
      "Epoch 20/100\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0459 - accuracy: 0.9838\n",
      "Epoch 21/100\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0459 - accuracy: 0.9839\n",
      "Epoch 22/100\n",
      "2958/2958 [==============================] - 25s 8ms/step - loss: 0.0459 - accuracy: 0.9839\n",
      "Epoch 23/100\n",
      "2958/2958 [==============================] - 38s 13ms/step - loss: 0.0454 - accuracy: 0.9839\n",
      "Epoch 24/100\n",
      "2958/2958 [==============================] - 23s 8ms/step - loss: 0.0456 - accuracy: 0.9843\n",
      "Epoch 25/100\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0457 - accuracy: 0.9840\n",
      "Epoch 26/100\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0451 - accuracy: 0.9841\n",
      "Epoch 27/100\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0453 - accuracy: 0.9839\n",
      "Epoch 28/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0450 - accuracy: 0.9841\n",
      "Epoch 29/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0451 - accuracy: 0.9843\n",
      "Epoch 30/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0448 - accuracy: 0.9842\n",
      "Epoch 31/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0447 - accuracy: 0.9843\n",
      "Epoch 32/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0447 - accuracy: 0.9842\n",
      "Epoch 33/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0449 - accuracy: 0.9843\n",
      "Epoch 34/100\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0446 - accuracy: 0.9842\n",
      "Epoch 35/100\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0443 - accuracy: 0.9842\n",
      "Epoch 36/100\n",
      "2958/2958 [==============================] - 23s 8ms/step - loss: 0.0446 - accuracy: 0.9844\n",
      "Epoch 37/100\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0446 - accuracy: 0.9842\n",
      "Epoch 38/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0445 - accuracy: 0.9845\n",
      "Epoch 39/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0444 - accuracy: 0.9844\n",
      "Epoch 40/100\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0443 - accuracy: 0.9844\n",
      "Epoch 41/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0444 - accuracy: 0.9845\n",
      "Epoch 42/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0442 - accuracy: 0.9845\n",
      "Epoch 43/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0440 - accuracy: 0.9844\n",
      "Epoch 44/100\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0440 - accuracy: 0.9843\n",
      "Epoch 45/100\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0442 - accuracy: 0.9846\n",
      "Epoch 46/100\n",
      "2958/2958 [==============================] - 22s 8ms/step - loss: 0.0440 - accuracy: 0.9846\n",
      "Epoch 47/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0445 - accuracy: 0.9846\n",
      "Epoch 48/100\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0442 - accuracy: 0.9846\n",
      "Epoch 49/100\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0440 - accuracy: 0.9843\n",
      "Epoch 50/100\n",
      "2958/2958 [==============================] - 22s 8ms/step - loss: 0.0439 - accuracy: 0.9846\n",
      "Epoch 51/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0436 - accuracy: 0.9846\n",
      "Epoch 52/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0436 - accuracy: 0.9847\n",
      "Epoch 53/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0437 - accuracy: 0.9848\n",
      "Epoch 54/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0437 - accuracy: 0.9848\n",
      "Epoch 55/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0441 - accuracy: 0.9845\n",
      "Epoch 56/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0433 - accuracy: 0.9848\n",
      "Epoch 57/100\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0436 - accuracy: 0.9849\n",
      "Epoch 58/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0435 - accuracy: 0.9849\n",
      "Epoch 59/100\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0435 - accuracy: 0.9845\n",
      "Epoch 60/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0433 - accuracy: 0.9847\n",
      "Epoch 61/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0436 - accuracy: 0.9848\n",
      "Epoch 62/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0435 - accuracy: 0.9847\n",
      "Epoch 63/100\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0435 - accuracy: 0.9848\n",
      "Epoch 64/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0436 - accuracy: 0.9846\n",
      "Epoch 65/100\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0433 - accuracy: 0.9848\n",
      "Epoch 66/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0432 - accuracy: 0.9847\n",
      "Epoch 67/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0434 - accuracy: 0.9850\n",
      "Epoch 68/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0428 - accuracy: 0.9849\n",
      "Epoch 69/100\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0433 - accuracy: 0.9848\n",
      "Epoch 70/100\n",
      "2958/2958 [==============================] - 2068s 699ms/step - loss: 0.0429 - accuracy: 0.9851\n",
      "Epoch 71/100\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0432 - accuracy: 0.9851\n",
      "Epoch 72/100\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0430 - accuracy: 0.9847\n",
      "Epoch 73/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0429 - accuracy: 0.9850\n",
      "Epoch 74/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0425 - accuracy: 0.9851\n",
      "Epoch 75/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0428 - accuracy: 0.9853\n",
      "Epoch 76/100\n",
      "2958/2958 [==============================] - 19s 6ms/step - loss: 0.0425 - accuracy: 0.9851\n",
      "Epoch 77/100\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0426 - accuracy: 0.9850\n",
      "Epoch 78/100\n",
      "2958/2958 [==============================] - 18s 6ms/step - loss: 0.0428 - accuracy: 0.9848\n",
      "Epoch 79/100\n",
      "2958/2958 [==============================] - 17s 6ms/step - loss: 0.0427 - accuracy: 0.9852\n",
      "Epoch 80/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0428 - accuracy: 0.9850\n",
      "Epoch 81/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0422 - accuracy: 0.9852\n",
      "Epoch 82/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0425 - accuracy: 0.9850\n",
      "Epoch 83/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0425 - accuracy: 0.9850\n",
      "Epoch 84/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0428 - accuracy: 0.9849\n",
      "Epoch 85/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0424 - accuracy: 0.9851\n",
      "Epoch 86/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0426 - accuracy: 0.9851\n",
      "Epoch 87/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0425 - accuracy: 0.9850\n",
      "Epoch 88/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0423 - accuracy: 0.9853\n",
      "Epoch 89/100\n",
      "2958/2958 [==============================] - 21s 7ms/step - loss: 0.0423 - accuracy: 0.9853\n",
      "Epoch 90/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0425 - accuracy: 0.9851\n",
      "Epoch 91/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0425 - accuracy: 0.9850\n",
      "Epoch 92/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0424 - accuracy: 0.9853\n",
      "Epoch 93/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0424 - accuracy: 0.9850\n",
      "Epoch 94/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0422 - accuracy: 0.9852\n",
      "Epoch 95/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0419 - accuracy: 0.9854\n",
      "Epoch 96/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0423 - accuracy: 0.9850\n",
      "Epoch 97/100\n",
      "2958/2958 [==============================] - 22s 7ms/step - loss: 0.0423 - accuracy: 0.9852\n",
      "Epoch 98/100\n",
      "2958/2958 [==============================] - 19s 7ms/step - loss: 0.0421 - accuracy: 0.9854\n",
      "Epoch 99/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0421 - accuracy: 0.9852\n",
      "Epoch 100/100\n",
      "2958/2958 [==============================] - 20s 7ms/step - loss: 0.0422 - accuracy: 0.9852\n",
      "5914/5914 [==============================] - 15s 3ms/step - loss: 0.0410 - accuracy: 0.9857\n",
      "1th Fold :\n",
      "accuracy: 98.57%\n",
      "------------------------------------------------------------------\n",
      "Average validation accuracy : \n",
      "98.56% (+/- 0.01%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "i=0\n",
    "for train, test in kfold.split(x_train, y_train):\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Convolution1D(64, 3, padding=\"same\",activation=\"relu\",input_shape=(44,1)))\n",
    "    cnn.add(MaxPooling1D(pool_size=(2)))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation=\"relu\"))\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(Dense(2, activation=\"sigmoid\"))\n",
    "    # define optimizer and objective, compile cnn\n",
    "\n",
    "\n",
    "    cnn.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\",metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    x_tn = x_train.iloc[train]\n",
    "    y_tn = y_train.iloc[train]\n",
    "    y_tn = pd.get_dummies(y_tn)\n",
    "    x_ts = x_train.iloc[test]\n",
    "    y_ts = y_train.iloc[test]\n",
    "    y_ts = pd.get_dummies(y_ts)\n",
    "    \n",
    "    x_tn1 = x_tn.to_numpy()\n",
    "    x_tn1 = np.reshape(x_tn1, (x_tn1.shape[0],x_tn1.shape[1],1))\n",
    "    \n",
    "    \n",
    "    x_ts1 = x_ts.to_numpy()\n",
    "    x_ts1 = np.reshape(x_ts1, (x_ts1.shape[0],x_ts1.shape[1],1))\n",
    "    \n",
    "    cnn.fit(x_tn1, y_tn, epochs=100,batch_size=64,verbose=1)\n",
    "    scores = cnn.evaluate(x_ts1, y_ts, verbose=1)\n",
    "    print(str(i)+\"th Fold :\")\n",
    "    print(\"%s: %.2f%%\" % (cnn.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    i = i+1\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Average validation accuracy : \")   \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fc247389",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x_train.to_numpy()\n",
    "x_tr = np.reshape(x_tr, (x_tr.shape[0], x_tr.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "02e39e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts = x_test.to_numpy()\n",
    "x_ts = np.reshape(x_ts, (x_ts.shape[0], x_ts.shape[1],1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ecf740d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = pd.get_dummies(y_train)\n",
    "y_ts = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45d3c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11829/11829 [==============================] - 31s 3ms/step - loss: 0.0398 - accuracy: 0.9859\n",
      "2958/2958 [==============================] - 8s 3ms/step - loss: 0.0410 - accuracy: 0.9851\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = cnn.evaluate(x_tr, y_tr, verbose=1)\n",
    "_, test_acc = cnn.evaluate(x_ts, y_ts, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36e38bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.59126806259155\n",
      "Test accuracy: 98.50885272026062\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: \"+str(train_acc*100))\n",
    "print(\"Test accuracy: \"+str(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc518652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2958/2958 [==============================] - 7s 2ms/step\n",
      "2958/2958 [==============================] - 7s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_probs = cnn.predict(x_ts, verbose=1).ravel()\n",
    "#y_classes = cnn.predict_classes(x_ts, verbose=1)\n",
    "#y_classes = (cnn.predict(x_ts) > 0.5).astype(\"int32\")\n",
    "y_classes = np.argmax(cnn.predict(x_ts), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "692e9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.get_dummies(y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ef77bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     48236\n",
      "           1       0.99      0.98      0.98     46389\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     94625\n",
      "   macro avg       0.99      0.99      0.99     94625\n",
      "weighted avg       0.99      0.99      0.99     94625\n",
      " samples avg       0.99      0.99      0.99     94625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_ts,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f592574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08a58134",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_le = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "95417666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test_le, y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0a951d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86c221d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "693c1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9850248373561499\n"
     ]
    }
   ],
   "source": [
    "print(auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d83af47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(fpr_keras, tpr_keras)\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734145e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save(\"NIDS.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
